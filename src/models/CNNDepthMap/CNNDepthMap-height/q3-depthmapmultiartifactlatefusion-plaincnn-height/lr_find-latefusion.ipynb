{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import random\n",
    "from typing import List\n",
    "\n",
    "import glob2 as glob\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from azureml.core import Experiment, Workspace\n",
    "from azureml.core.run import Run\n",
    "from tensorflow.keras import callbacks, layers, models\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()) / 'src'))\n",
    "\n",
    "from config import CONFIG, DATASET_MODE_DOWNLOAD, DATASET_MODE_MOUNT\n",
    "from constants import DATA_DIR_ONLINE_RUN, REPO_DIR\n",
    "from model import create_base_cnn, create_head, load_base_cgm_model\n",
    "from preprocessing import create_multiartifact_paths, tf_load_pickle, tf_augment_sample\n",
    "from utils import download_dataset, get_dataset_path  # TODO import from model_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make experiment reproducible\n",
    "tf.random.set_seed(CONFIG.SPLIT_SEED)\n",
    "random.seed(CONFIG.SPLIT_SEED)\n",
    "\n",
    "# Get the current run.\n",
    "run = Run.get_context()\n",
    "\n",
    "DATA_DIR = REPO_DIR / 'data' if run.id.startswith(\"OfflineRun\") else Path(\".\")\n",
    "print(f\"DATA_DIR: {DATA_DIR}\")\n",
    "\n",
    "# Offline run. Download the sample dataset and run locally. Still push results to Azure.\n",
    "if run.id.startswith(\"OfflineRun\"):\n",
    "    print(\"Running in offline mode...\")\n",
    "\n",
    "    # Access workspace.\n",
    "    print(\"Accessing workspace...\")\n",
    "    workspace = Workspace.from_config()\n",
    "    experiment = Experiment(workspace, \"training-junkyard\")\n",
    "    run = experiment.start_logging(outputs=None, snapshot_directory=None)\n",
    "\n",
    "    dataset_name = CONFIG.DATASET_NAME_LOCAL\n",
    "    dataset_path = get_dataset_path(DATA_DIR, dataset_name)\n",
    "    download_dataset(workspace, dataset_name, dataset_path)\n",
    "\n",
    "# Online run. Use dataset provided by training notebook.\n",
    "else:\n",
    "    print(\"Running in online mode...\")\n",
    "    experiment = run.experiment\n",
    "    workspace = experiment.workspace\n",
    "\n",
    "    dataset_name = CONFIG.DATASET_NAME\n",
    "\n",
    "    # Mount or download\n",
    "    if CONFIG.DATASET_MODE == DATASET_MODE_MOUNT:\n",
    "        dataset_path = run.input_datasets[\"dataset\"]\n",
    "    elif CONFIG.DATASET_MODE == DATASET_MODE_DOWNLOAD:\n",
    "        dataset_path = get_dataset_path(DATA_DIR_ONLINE_RUN, dataset_name)\n",
    "        download_dataset(workspace, dataset_name, dataset_path)\n",
    "    else:\n",
    "        raise NameError(f\"Unknown DATASET_MODE: {CONFIG.DATASET_MODE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the QR-code paths.\n",
    "dataset_scans_path = os.path.join(dataset_path, \"scans\")\n",
    "print(\"Dataset path:\", dataset_scans_path)\n",
    "# print(glob.glob(os.path.join(dataset_scans_path, \"*\"))) # Debug\n",
    "print(\"Getting QR-code paths...\")\n",
    "qrcode_paths = glob.glob(os.path.join(dataset_scans_path, \"*\"))\n",
    "print(\"qrcode_paths: \", len(qrcode_paths))\n",
    "assert len(qrcode_paths) != 0\n",
    "\n",
    "# Shuffle and split into train and validate.\n",
    "random.seed(CONFIG.SPLIT_SEED)\n",
    "random.shuffle(qrcode_paths)\n",
    "split_index = int(len(qrcode_paths) * 0.8)\n",
    "qrcode_paths_training = qrcode_paths[:split_index]\n",
    "\n",
    "qrcode_paths_validate = qrcode_paths[split_index:]\n",
    "qrcode_paths_activation = random.choice(qrcode_paths_validate)\n",
    "qrcode_paths_activation = [qrcode_paths_activation]\n",
    "\n",
    "del qrcode_paths\n",
    "\n",
    "# Show split.\n",
    "print(\"Paths for training:\")\n",
    "print(\"\\t\" + \"\\n\\t\".join(qrcode_paths_training))\n",
    "print(\"Paths for validation:\")\n",
    "print(\"\\t\" + \"\\n\\t\".join(qrcode_paths_validate))\n",
    "print(\"Paths for activation:\")\n",
    "print(\"\\t\" + \"\\n\\t\".join(qrcode_paths_activation))\n",
    "\n",
    "print(len(qrcode_paths_training))\n",
    "print(len(qrcode_paths_validate))\n",
    "\n",
    "assert len(qrcode_paths_training) > 0 and len(qrcode_paths_validate) > 0\n",
    "\n",
    "\n",
    "def create_samples(qrcode_paths: List[str]) -> List[List[str]]:\n",
    "    samples = []\n",
    "    for qrcode_path in sorted(qrcode_paths):\n",
    "        for code in CONFIG.CODES_FOR_POSE_AND_SCANSTEP:\n",
    "            p = os.path.join(qrcode_path, code)\n",
    "            new_samples = create_multiartifact_paths(p, CONFIG.N_ARTIFACTS)\n",
    "            samples.extend(new_samples)\n",
    "    return samples\n",
    "\n",
    "\n",
    "paths_training = create_samples(qrcode_paths_training)\n",
    "print(f\"Samples for training: {len(paths_training)}\")\n",
    "\n",
    "paths_validate = create_samples(qrcode_paths_validate)\n",
    "print(f\"Samples for validate: {len(paths_validate)}\")\n",
    "\n",
    "paths_activate = create_samples(qrcode_paths_activation)\n",
    "print(f\"Samples for activate: {len(paths_activate)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for training.\n",
    "paths = paths_training  # list\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)  # TensorSliceDataset  # List[ndarray[str]]\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.repeat(CONFIG.N_REPEAT_DATASET)\n",
    "dataset = dataset.map(\n",
    "    lambda path: tf_load_pickle(paths=path),\n",
    "    tf.data.experimental.AUTOTUNE\n",
    ")  # (240,180,5), (1,)\n",
    "\n",
    "dataset = dataset.map(tf_augment_sample, tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(CONFIG.SHUFFLE_BUFFER_SIZE)\n",
    "dataset_training = dataset\n",
    "\n",
    "# Create dataset for validation.\n",
    "# Note: No shuffle necessary.\n",
    "paths = paths_validate\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset_norm = dataset.map(lambda path: tf_load_pickle(path), tf.data.experimental.AUTOTUNE)\n",
    "dataset_norm = dataset_norm.cache()\n",
    "dataset_norm = dataset_norm.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_validation = dataset_norm\n",
    "del dataset_norm\n",
    "\n",
    "# Create dataset for activation\n",
    "# paths = paths_activate\n",
    "# dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "# dataset_norm = dataset.map(lambda path: tf_load_pickle(path), tf.data.experimental.AUTOTUNE)\n",
    "# dataset_norm = dataset_norm.cache()\n",
    "# dataset_norm = dataset_norm.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "# dataset_activation = dataset_norm\n",
    "# del dataset_norm\n",
    "\n",
    "# Note: Now the datasets are prepared.\n",
    "\n",
    "\n",
    "def download_pretrained_model(output_model_fpath):\n",
    "    print(f\"Downloading pretrained model from {CONFIG.PRETRAINED_RUN}\")\n",
    "    previous_experiment = Experiment(workspace=workspace, name=CONFIG.PRETRAINED_EXPERIMENT)\n",
    "    previous_run = Run(previous_experiment, CONFIG.PRETRAINED_RUN)\n",
    "    previous_run.download_file(\"outputs/best_model.h5\", output_model_fpath)\n",
    "\n",
    "\n",
    "def get_base_model():\n",
    "    if CONFIG.PRETRAINED_RUN:\n",
    "        model_fpath = DATA_DIR / \"pretrained/\" / CONFIG.PRETRAINED_RUN / \"best_model.h5\"\n",
    "        if not os.path.exists(model_fpath):\n",
    "            download_pretrained_model(model_fpath)\n",
    "        print(f\"Loading pretrained model from {model_fpath}\")\n",
    "        base_model = load_base_cgm_model(model_fpath, should_freeze=CONFIG.SHOULD_FREEZE_BASE)\n",
    "    else:\n",
    "        input_shape = (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1)\n",
    "        base_model = create_base_cnn(input_shape, dropout=CONFIG.USE_DROPOUT)  # output_shape: (128,)\n",
    "    return base_model\n",
    "\n",
    "\n",
    "# Create the base model\n",
    "base_model = get_base_model()\n",
    "assert base_model.output_shape == (None, 128)\n",
    "\n",
    "# Create the head\n",
    "head_input_shape = (128 * CONFIG.N_ARTIFACTS,)\n",
    "head_model = create_head(head_input_shape, dropout=CONFIG.USE_CROPOUT)\n",
    "\n",
    "# Implement artifact flow through the same model\n",
    "model_input = layers.Input(\n",
    "    shape=(CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, CONFIG.N_ARTIFACTS)\n",
    ")\n",
    "\n",
    "features_list = []\n",
    "for i in range(CONFIG.N_ARTIFACTS):\n",
    "    features_part = model_input[:, :, :, i:i + 1]\n",
    "    features_part = base_model(features_part)\n",
    "    features_list.append(features_part)\n",
    "\n",
    "concatenation = tf.keras.layers.concatenate(features_list, axis=-1)\n",
    "assert concatenation.shape.as_list() == tf.TensorShape((None, 128 * CONFIG.N_ARTIFACTS)).as_list()\n",
    "model_output = head_model(concatenation)\n",
    "\n",
    "model = models.Model(model_input, model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mae\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lr_finder import LRFinder\n",
    "lr_finder = LRFinder()\n",
    "_ = model.fit(dataset_training.batch(CONFIG.BATCH_SIZE), epochs=5, callbacks=[lr_finder], verbose=2)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(dataset_training) # 95k=2059"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
